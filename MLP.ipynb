{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNISTdata():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    y_train: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    y_test: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the train files\n",
    "    df = None\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'mnistdata/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    train_data = df.to_numpy()\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    #load test files\n",
    "    df = None\n",
    "    \n",
    "    y_test = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'mnistdata/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    test_data = df.to_numpy()\n",
    "    y_test = np.array( y_test )\n",
    "    \n",
    "    return train_data, test_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "\n",
    "def load_pickle(f):\n",
    "    return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000,3072)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def get_proper_Ydata(Y):\n",
    "    K = np.amax(Y) + 1\n",
    "    N = Y.shape[0]\n",
    "    properY = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        clf = Y[i]\n",
    "        properY[i][clf] = 1\n",
    "    return properY\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\"\n",
    "    Load the CIFAR dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    Xtrain:the matrix with the training data\n",
    "    Xtest: the matrix with the data that will be used for testing\n",
    "    ytrain: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    ytest: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = ROOT + 'data_batch_%d' % (b, )\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtrain = np.concatenate(xs)\n",
    "    Ytrain = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xtest, Ytest = load_CIFAR_batch(ROOT + 'test_batch')\n",
    "    return Xtrain, get_proper_Ydata(Ytrain), Xtest, get_proper_Ydata(Ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Activation functions and their derivatives for this project.\"\"\"\n",
    "def h1(a):\n",
    "    return np.log(1+np.exp(a))\n",
    "\n",
    "def h2(a):\n",
    "    expa = np.exp(a)\n",
    "    exp_a = np.exp(-a)\n",
    "    return (expa - exp_a)/ (expa + exp_a)\n",
    "\n",
    "def h3(a):\n",
    "    return np.cos(a)\n",
    "\n",
    "def dh1(a):\n",
    "    return np.exp(a)/(np.exp(a)+1)\n",
    "\n",
    "def dh2(a):\n",
    "    return 1 - np.tanh(a)**2\n",
    "\n",
    "def dh3(a):\n",
    "    return -np.sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyFunction(h, x):\n",
    "    \"\"\"\n",
    "    Apply one of the activation functions (or derivatives on an array).\n",
    "    input: h - function used\n",
    "           x - array we want to apply the function on\n",
    "    return: the modified array\n",
    "    \"\"\"\n",
    "    return np.array([h(xi) for xi in x])\n",
    "\n",
    "def normalize(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Normalize the dataset from pixel values to float.\n",
    "    input: X_train - X train data array\n",
    "           X_test - X test data array\n",
    "    return: tuple (X_train_norm, X_test_norm) of the normalized arrays\n",
    "    \"\"\"\n",
    "    X_train_norm = X_train.astype(float)/255\n",
    "    X_test_norm = X_test.astype(float)/255\n",
    "    #try changing astype(float) tp /255.0\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "def softmax(x, k):\n",
    "    \"\"\"\n",
    "    Calculates softmax for the k row of given array x.\n",
    "    input: x - array that we want to use\n",
    "           k - row of the array (the numerator of the softmax)\n",
    "    return: float\n",
    "    \"\"\"\n",
    "    #set x_exp as the array with the exponential values of x\n",
    "    x_exp = np.exp( x )\n",
    "    return ( x_exp[k] / np.sum(x_exp, axis = 0) )\n",
    "\n",
    "def calculateNorm(W1, W2):\n",
    "    \"\"\"\n",
    "    Calculates the weight norm, given the 2 weight tables.\n",
    "    input: W1 - array of weights 1 of dimensions Mx(D+1)\n",
    "           W2 - array of weights 2 of dimensions Kx(M+1)\n",
    "    return: float, the calculated norm\n",
    "    \"\"\"\n",
    "    #first we calculate each norm seperately and then sum up the 2 norms\n",
    "    normW1= np.sum(np.square(W1))\n",
    "    normW2 = np.sum(np.square(W2))\n",
    "    return np.sum(np.array([normW1, normW2]))\n",
    "\n",
    "def initialize(fin, fout):\n",
    "    \"\"\"\n",
    "    Initializes an array of size/shape (fin, fout) using the Glorot distribution.\n",
    "    input - fin, fout: array dimensions\n",
    "    return - initialized array W\n",
    "    \"\"\"\n",
    "    glrt = np.sqrt(6/ (fin+ fout))\n",
    "    W = np.random.uniform(-glrt, glrt, (fin, fout))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_softmax(W1, W2, X, t, lamda, M, choice):\n",
    "    \"\"\"\n",
    "    Calculates cost and derivatives of weights for the weight update.\n",
    "    input: W1 - weight array of dimensions Mx(D+1)\n",
    "           W2 - weight array of dimensions Kx(M+1)\n",
    "           X  - X data array of dimensions Nx(D+1), where N could be batch size\n",
    "           t  - t data array of dimensions NxK, likewise\n",
    "           lamda - the positive regularizarion parameter\n",
    "           M  - integer which represents the number of hidden units of MLP\n",
    "           choice - integer (1, 2 or 3) which defines the desired activation function\n",
    "    return: Cost E - float\n",
    "            gradEw1 - array of dimensions Mx(D+1)\n",
    "            gradEw2 - array of dimensions Kx(M+1)\n",
    "    \"\"\"\n",
    "    #z, dz = h(X.dot(w1.T)) -> Nx(M+1)\n",
    "    #softmax( z.dot(w2.T))\n",
    "    #initialize cost\n",
    "    E = 0\n",
    "    N, D = X.shape\n",
    "    K = t.shape[1]\n",
    "    \n",
    "    #initialize Z and dZ (derivative of Z) arrays\n",
    "    Z = np.zeros((N, M))\n",
    "    dZ = np.zeros((N, M))\n",
    "    \n",
    "    #calculate the values and fill in the previously mentioned tables\n",
    "    #for each row/ example of the X array data, using the chosen function\n",
    "    for n_z in range(N):\n",
    "        if choice == 1:\n",
    "            Z[n_z] = applyFunction(h1, np.dot(X[n_z], W1.T))\n",
    "            dZ[n_z] = applyFunction(dh1, np.dot(X[n_z], W1.T))\n",
    "        elif choice == 2:\n",
    "            Z[n_z] = applyFunction(h2, np.dot(X[n_z], W1.T))\n",
    "            dZ[n_z] = applyFunction(dh2, np.dot(X[n_z], W1.T))\n",
    "        elif choice == 3:\n",
    "            Z[n_z] = applyFunction(h3, np.dot(X[n_z], W1.T))\n",
    "            dZ[n_z] = applyFunction(dh3, np.dot(X[n_z], W1.T))\n",
    "    #print(Z)\n",
    "    #print(dZ)\n",
    "    #add bias column as first column of Z table after the calculation of all the other values\n",
    "    Z = np.hstack( (np.ones((Z.shape[0], 1)), Z))\n",
    "    \n",
    "    #initialize y table and fill it in with the softmax values\n",
    "    Y = np.zeros((N, K))\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            Y[n][k] = softmax( np.dot(Z[n],W2.T), k )\n",
    "            #add tnk*log(ynk) to the cost\n",
    "            E += t[n][k] * np.log( Y[n][k] )\n",
    "    \n",
    "    #substract the lamda*norm/2 from cost\n",
    "    E -= lamda * calculateNorm(W1, W2) / 2\n",
    "    \n",
    "    #calculate the derivatives of each weight table (skipping the bias column of W2 on the W1 calculation\n",
    "    #                                                so that the arrays can match sizes on calculation)\n",
    "    gradEw1 = np.dot( (np.multiply( np.dot( (t-Y) , W2[:,1:]), dZ)).T, X) - lamda * W1\n",
    "    gradEw2 = np.dot( (t-Y).T, Z ) - lamda * W2\n",
    "    \n",
    "    return E, gradEw1, gradEw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_softmax_train(t, X, lamda, Winit1, Winit2, options):\n",
    "    \"\"\"inputs :\n",
    "           t  - t data array of dimensions NxK, likewise\n",
    "           X  - X data array of dimensions Nx(D+1), where N is the total number of examples\n",
    "           lamda - the positive regularizarion parameter\n",
    "           Winit1 - initialized weight array of dimensions Mx(D+1)\n",
    "           Winit2 - initialized weight array of dimensions Kx(M+1)\n",
    "      options: options(1) is the maximum number of iterations\n",
    "               options(2) is the tolerance\n",
    "               options(3) is the learning rate eta\n",
    "               options(4) is the batch size\n",
    "               options(5) is the activation function code choice\n",
    "    outputs :\n",
    "      W1: the trained W1 \n",
    "      W2: the trained W2\n",
    "      costs: the cost list\"\"\"\n",
    "    \n",
    "    #shuffle the data by unison\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    t = t[indices]\n",
    "    \n",
    "    W1 = Winit1\n",
    "    W2 = Winit2\n",
    "    \n",
    "    # Maximum number of iteration of gradient ascend\n",
    "    _iter = options[0]\n",
    "\n",
    "    # Tolerance\n",
    "    tol = options[1]\n",
    "\n",
    "    # Learning rate\n",
    "    eta = options[2]\n",
    "    \n",
    "    #batch size\n",
    "    batch_size = options[3]\n",
    "    \n",
    "    #activation function code\n",
    "    choice = options[4]\n",
    "    \n",
    "    #number of hidden units\n",
    "    M = W1.shape[0]\n",
    "    \n",
    "    #initialize the previously calculated cost to - infinity\n",
    "    Ewold = -np.inf\n",
    "    \n",
    "    #initialize costs list\n",
    "    costs = []\n",
    "    #for each batch\n",
    "    for i in range( 0, X.shape[0] , batch_size):\n",
    "        #calculate cost from grad ascend for slice of the dataset equal to the batch size\n",
    "        Ew, gradEw1, gradEw2 = cost_grad_softmax(W1, W2, X[i:i+batch_size], t[i:i+batch_size], lamda, M, choice)\n",
    "        # save cost\n",
    "        costs.append(Ew)\n",
    "        # Show the current cost function on screen\n",
    "        print('Iteration : %d, Cost function :%f' % (i/batch_size + 1, Ew))\n",
    "\n",
    "        # Break if you achieve the desired accuracy in the cost function\n",
    "        if np.abs(Ew - Ewold) < tol:\n",
    "            break\n",
    "        \n",
    "        #if maximum number of iterations reached, stop\n",
    "        _iter = _iter - 1\n",
    "        if (_iter == 0):\n",
    "            break\n",
    "        # Update parameters based on gradient ascend\n",
    "        W1 = W1 + eta * gradEw1\n",
    "        W2 = W2 + eta * gradEw2\n",
    "        Ewold = Ew\n",
    "\n",
    "    return W1, W2, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcheck_softmax(Winit1, Winit2, X, t, lamda, M, choice):\n",
    "    \n",
    "    W1 = np.random.rand(*Winit1.shape)\n",
    "    W2 = np.random.rand(*Winit2.shape)\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    _list = np.random.randint(X.shape[0], size=5)\n",
    "    x_sample = np.array(X[_list, :])\n",
    "    t_sample = np.array(t[_list, :])\n",
    "    \n",
    "    Ew, gradEw1, gradEw2 = cost_grad_softmax(W1, W2, x_sample, t_sample, lamda, M, choice)\n",
    "    \n",
    "    #print( \"gradEw shape: \", gradEw.shape )\n",
    "    \n",
    "    numericalGrad1 = np.zeros(gradEw1.shape)\n",
    "    numericalGrad2 = np.zeros(gradEw2.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for i in range(numericalGrad1.shape[0]):\n",
    "        for j in range(numericalGrad1.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w[k,d]\n",
    "            w_tmp1 = np.copy(W1)\n",
    "            w_tmp2 = np.copy(W2)\n",
    "            w_tmp1[i, j] += epsilon\n",
    "            e_plus1, _, _= cost_grad_softmax(w_tmp1, w_tmp2, x_sample, t_sample, lamda, M, choice)\n",
    "\n",
    "            #subtract epsilon to the w[k,d]\n",
    "            w_tmp1 = np.copy(W1)\n",
    "            w_tmp2 = np.copy(W2)\n",
    "            w_tmp1[i, j] -= epsilon\n",
    "            e_minus1, _, _ = cost_grad_softmax( w_tmp1, w_tmp2, x_sample, t_sample, lamda, M, choice)\n",
    "            \n",
    "            #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad1[i, j] = (e_plus1 - e_minus1) / (2 * epsilon)\n",
    "    print(\"finished w1\")        \n",
    "    for k in range(numericalGrad2.shape[0]):\n",
    "        for l in range(numericalGrad2.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w[k,d]\n",
    "            w_tmp1 = np.copy(W1)\n",
    "            w_tmp2 = np.copy(W2)\n",
    "            w_tmp2[k, l] += epsilon\n",
    "            e_plus2, _, _= cost_grad_softmax(w_tmp1, w_tmp2, x_sample, t_sample, lamda, M, choice)\n",
    "\n",
    "            #subtract epsilon to the w[k,d]\n",
    "            w_tmp1 = np.copy(W1)\n",
    "            w_tmp2 = np.copy(W2)\n",
    "            w_tmp2[k, l] -= epsilon\n",
    "            e_minus2, _, _ = cost_grad_softmax(w_tmp1, w_tmp2, x_sample, t_sample, lamda, M, choice)\n",
    "            \n",
    "            #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad2[k, l] = (e_plus2 - e_minus2) / (2 * epsilon)\n",
    "    print(\"finished w2\")    \n",
    "    return ( gradEw1, gradEw2, numericalGrad1, numericalGrad2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_softmax_test(W1, W2, X_test, Y_test, K, choice):\n",
    "    \n",
    "    if choice == 1:\n",
    "        ztest = applyFunction(h1, np.dot(X_test, W1.T))\n",
    "    elif choice == 2:\n",
    "        ztest = applyFunction(h2, np.dot(X_test, W1.T))\n",
    "    elif choice == 3:\n",
    "        ztest = applyFunction(h3, np.dot(X_test, W1.T))\n",
    "    \n",
    "    ztest = np.hstack( (np.ones((ztest.shape[0], 1)), ztest))\n",
    "    \n",
    "    ytest = np.zeros((X_test.shape[0], K))\n",
    "    for i in range(X_test.shape[0]):\n",
    "        for k in range(K):\n",
    "            ytest[i][k] = softmax( np.dot(ztest[i],W2.T), k )\n",
    "            \n",
    "    prediction = np.argmax(ytest, 1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ALL cells above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if you want to get the data from MNIST dataset\n",
    "X_train, X_test, Y_train, Y_test = load_MNISTdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if you want to get the data from CIFAR dataset\n",
    "X_train, Y_train, X_test, Y_test = load_CIFAR10('cifar-10-batches-py/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells bellow to train and predict using the MLP after you've ran ONE of the load data cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "X_train, X_test = normalize(X_train, X_test)\n",
    "#set up the parameters\n",
    "N, D = X_train.shape\n",
    "K = 10\n",
    "lamda = 0.01\n",
    "M = 100\n",
    "options = [1000, 1e-6, 0.005, 100, 3]\n",
    "#initialize weights and add bias column to the data\n",
    "Winit1 = initialize(M, D+1)\n",
    "Winit2 = initialize(K, M)\n",
    "Winit2 = np.hstack( (np.ones((Winit2.shape[0], 1)), Winit2))\n",
    "X_train = np.hstack( (np.ones((X_train.shape[0],1) ), X_train) )\n",
    "X_test = np.hstack( (np.ones((X_test.shape[0],1) ), X_test) )\n",
    "#train the MLP\n",
    "W1, W2, costs = ml_softmax_train(Y_train, X_train, lamda, Winit1, Winit2, options)\n",
    "#predict\n",
    "prediction = ml_softmax_test(W1, W2, X_test, Y_test, K ,options[4])\n",
    "#check accuracy of predictions\n",
    "print(\"Accuracy: \" + str(np.mean( prediction == np.argmax(Y_test,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot costs by iteration\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(format(options[2], 'f')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell bellow to get the difference estimate after you've ran ONE of the load data cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradEw1, gradEw2, numericalGrad1, numericalGrad2 = gradcheck_softmax(Winit1, Winit2, X_train, Y_train, lamda, M, 3)\n",
    "print( \"The difference estimate for gradient of w1 is : \", np.max(np.abs(gradEw1 - numericalGrad1)) )\n",
    "print( \"The difference estimate for gradient of w2 is : \", np.max(np.abs(gradEw2 - numericalGrad2)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
